{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nAs of June 2020, there are eight planets in our Solar System: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.',\n",
       " 'As of now, there are eight recognized planets in our solar system. They are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, there is ongoing debate among astronomers regarding the classification of Pluto as a planet, as it was reclassified as a dwarf planet in 2006 by the International Astronomical Union (IAU).')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.0 LLMs and Chat Models\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "# .env 파일 쓰려면 dotenv 사용하기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(\"How many planets are there?\")\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 18.000 chilometri.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1 Predict Messages\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a geography expert. And you only reply in Italian.\"\n",
    "    ),\n",
    "    AIMessage(content=\"Ciao, mi chiamo Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and Thailand. Also, what is your name?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand is approximately 16,000 kilometers (9,942 miles).'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2 Prompt Templates\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "import os\n",
    "# .env 파일 쓰려면 dotenv 사용하기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}\")\n",
    "\n",
    "prompt = template.format(country_a=\"Mexcio\", country_b=\"Thailand\")\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n",
    "template_message = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b}. Also, what is your name?\")\n",
    "])\n",
    "\n",
    "prompt = template_message.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a = \"Mexico\",\n",
    "    country_b = \"korea\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charizard', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3 OutputParser and LCEL\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the planets?\",\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content) # ['mercury', 'venus', 'earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune', 'pluto']\n",
    "\n",
    "chain = template | chat | CommaOutputParser() # | 로 chain 형성!\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\":5,\n",
    "    \"question\": \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! Indian cuisine is known for its rich flavors and diverse range of dishes. Here's a simple recipe for you to try:\n",
      "\n",
      "Butter Chicken (Murgh Makhani):\n",
      "Ingredients:\n",
      "- 500g boneless chicken, cut into pieces\n",
      "- 2 tablespoons butter\n",
      "- 1 onion, finely chopped\n",
      "- 2 teaspoons ginger-garlic paste\n",
      "- 2 teaspoons red chili powder\n",
      "- 1 teaspoon turmeric powder\n",
      "- 1 teaspoon garam masala\n",
      "- 1 cup tomato puree\n",
      "- 1/2 cup heavy cream\n",
      "- Salt to taste\n",
      "- Fresh coriander leaves for garnish\n",
      "\n",
      "Instructions:\n",
      "1. Heat butter in a pan over medium heat. Add the chopped onions and sauté until golden brown.\n",
      "2. Add ginger-garlic paste and sauté for another minute.\n",
      "3. Add red chili powder, turmeric powder, and garam masala. Mix well and cook for a minute.\n",
      "4. Add tomato puree and cook for 5-7 minutes until the oil separates from the mixture.\n",
      "5. Add the chicken pieces and cook until they are cooked through and tender.\n",
      "6. Stir in the heavy cream and season with salt. Simmer for another 5 minutes.\n",
      "7. Garnish with fresh coriander leaves and serve hot with naan bread or steamed rice.\n",
      "\n",
      "Remember, Indian cuisine offers a wide variety of dishes, so feel free to explore and experiment with different recipes and flavors. Enjoy your cooking!Butter Chicken, also known as Murgh Makhani, is a popular Indian dish that is traditionally made with chicken. However, as a vegetarian chef, I can suggest a delicious alternative to replace the chicken in this recipe.\n",
      "\n",
      "Instead of using chicken, you can use paneer, which is a type of Indian cheese. Paneer has a mild and creamy flavor that works well in rich and flavorful dishes like Butter Chicken. Here's how you can modify the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 500g paneer, cut into cubes\n",
      "- 2 tablespoons butter\n",
      "- 1 onion, finely chopped\n",
      "- 2 teaspoons ginger-garlic paste\n",
      "- 2 teaspoons red chili powder\n",
      "- 1 teaspoon turmeric powder\n",
      "- 1 teaspoon garam masala\n",
      "- 1 cup tomato puree\n",
      "- 1/2 cup heavy cream\n",
      "- Salt to taste\n",
      "- Fresh coriander leaves for garnish\n",
      "\n",
      "Instructions:\n",
      "1. Heat butter in a pan over medium heat. Add the chopped onions and sauté until golden brown.\n",
      "2. Add ginger-garlic paste and sauté for another minute.\n",
      "3. Add red chili powder, turmeric powder, and garam masala. Mix well and cook for a minute.\n",
      "4. Add tomato puree and cook for 5-7 minutes until the oil separates from the mixture.\n",
      "5. Add the paneer cubes and cook until they are heated through.\n",
      "6. Stir in the heavy cream and season with salt. Simmer for another 5 minutes.\n",
      "7. Garnish with fresh coriander leaves and serve hot with naan bread or steamed rice.\n",
      "\n",
      "By using paneer instead of chicken, you can still enjoy the rich and creamy flavors of Butter Chicken while keeping it vegetarian."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Butter Chicken, also known as Murgh Makhani, is a popular Indian dish that is traditionally made with chicken. However, as a vegetarian chef, I can suggest a delicious alternative to replace the chicken in this recipe.\\n\\nInstead of using chicken, you can use paneer, which is a type of Indian cheese. Paneer has a mild and creamy flavor that works well in rich and flavorful dishes like Butter Chicken. Here's how you can modify the recipe:\\n\\nIngredients:\\n- 500g paneer, cut into cubes\\n- 2 tablespoons butter\\n- 1 onion, finely chopped\\n- 2 teaspoons ginger-garlic paste\\n- 2 teaspoons red chili powder\\n- 1 teaspoon turmeric powder\\n- 1 teaspoon garam masala\\n- 1 cup tomato puree\\n- 1/2 cup heavy cream\\n- Salt to taste\\n- Fresh coriander leaves for garnish\\n\\nInstructions:\\n1. Heat butter in a pan over medium heat. Add the chopped onions and sauté until golden brown.\\n2. Add ginger-garlic paste and sauté for another minute.\\n3. Add red chili powder, turmeric powder, and garam masala. Mix well and cook for a minute.\\n4. Add tomato puree and cook for 5-7 minutes until the oil separates from the mixture.\\n5. Add the paneer cubes and cook until they are heated through.\\n6. Stir in the heavy cream and season with salt. Simmer for another 5 minutes.\\n7. Garnish with fresh coriander leaves and serve hot with naan bread or steamed rice.\\n\\nBy using paneer instead of chicken, you can still enjoy the rich and creamy flavors of Butter Chicken while keeping it vegetarian.\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4 Chaining Chains\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"), \n",
    "    (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"), \n",
    "    (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain # Runnable map\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\": \"indian\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "I know this:\n",
      "Capital: Berlin\n",
      "Language: German\n",
      "Food: Bratwurst and Sauerkraut\n",
      "Currency: Euro"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\nI know this:\\nCapital: Berlin\\nLanguage: German\\nFood: Bratwurst and Sauerkraut\\nCurrency: Euro')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1 FewShotPromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What do you know about France?\", \n",
    "             \"answer\": \"\"\"\n",
    "             Here is what I know: \n",
    "             Capital: Paris \n",
    "             Language: French \n",
    "             Food: Wine and Cheese \n",
    "             Currency: Euro\n",
    "             \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "            \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\": \"Germany\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        Capital: Bangkok\n",
      "        Language: Thai\n",
      "        Food: Pad Thai and Tom Yum Soup\n",
      "        Currency: Thai Baht\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        Capital: Bangkok\\n        Language: Thai\\n        Food: Pad Thai and Tom Yum Soup\\n        Currency: Thai Baht\\n        ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI:\\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.3 LengthBasedExampleSelector\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrrg! Me favorite food be a good ol' plate o' fish and chips! The salty sea air makes me crave the taste of fresh fish, battered and fried to perfection. And don't forget the crispy golden chips to go along with it! It be a meal fit for a pirate like meself! Arg arg!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Arrrrg! Me favorite food be a good ol' plate o' fish and chips! The salty sea air makes me crave the taste of fresh fish, battered and fried to perfection. And don't forget the crispy golden chips to go along with it! It be a meal fit for a pirate like meself! Arg arg!\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.4 Serialization and Composiotion\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta\"\n",
      "  ]\n",
      "}\n",
      "To make Italian pasta, you will need the following ingredients:\n",
      "\n",
      "- 2 cups of all-purpose flour\n",
      "- 2 large eggs\n",
      "- 1/2 teaspoon of salt\n",
      "- Water (if needed)\n",
      "\n",
      "Here's a step-by-step guide to making Italian pasta:\n",
      "\n",
      "1. On a clean surface or in a large mixing bowl, pour the flour and create a well in the center.\n",
      "2. Crack the eggs into the well and add the salt.\n",
      "3. Using a fork, beat the eggs and gradually incorporate the flour from the sides of the well. Continue mixing until a dough starts to form.\n",
      "4. Once the dough becomes too stiff to mix with a fork, use your hands to knead it. If the dough is too dry, add a little water, one tablespoon at a time, until it comes together. If it's too sticky, add a little flour.\n",
      "5. Knead the dough for about 5-10 minutes until it becomes smooth and elastic.\n",
      "6. Shape the dough into a ball and cover it with a clean kitchen towel. Let it rest for at least 30 minutes to allow the gluten to relax.\n",
      "7. After resting, divide the dough into smaller portions. Flatten each portion with a rolling pin or a pasta machine until it reaches the desired thickness. You can make it as thin or thick as you prefer.\n",
      "8. Once the dough is rolled out, you can cut it into various pasta shapes like fettuccine, spaghetti, or lasagna sheets.\n",
      "9. If making long pasta like spaghetti or fettuccine, dust the dough with flour and roll it up loosely. Use a sharp knife to cut it into thin strips.\n",
      "10. If making lasagna sheets, cut the rolled-out dough into rectangular pieces of the desired size.\n",
      "11. Once the pasta is cut, you can cook it immediately in boiling salted water for about 2-5 minutes until al dente. Alternatively, you can let it dry for a few hours or overnight before cooking.\n",
      "12. Serve the cooked pasta with your favorite sauce, such as marinara, carbonara, or pesto.\n",
      "\n",
      "Enjoy your homemade Italian pasta!\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [25.83s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere's a step-by-step guide to making Italian pasta:\\n\\n1. On a clean surface or in a large mixing bowl, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, beat the eggs and gradually incorporate the flour from the sides of the well. Continue mixing until a dough starts to form.\\n4. Once the dough becomes too stiff to mix with a fork, use your hands to knead it. If the dough is too dry, add a little water, one tablespoon at a time, until it comes together. If it's too sticky, add a little flour.\\n5. Knead the dough for about 5-10 minutes until it becomes smooth and elastic.\\n6. Shape the dough into a ball and cover it with a clean kitchen towel. Let it rest for at least 30 minutes to allow the gluten to relax.\\n7. After resting, divide the dough into smaller portions. Flatten each portion with a rolling pin or a pasta machine until it reaches the desired thickness. You can make it as thin or thick as you prefer.\\n8. Once the dough is rolled out, you can cut it into various pasta shapes like fettuccine, spaghetti, or lasagna sheets.\\n9. If making long pasta like spaghetti or fettuccine, dust the dough with flour and roll it up loosely. Use a sharp knife to cut it into thin strips.\\n10. If making lasagna sheets, cut the rolled-out dough into rectangular pieces of the desired size.\\n11. Once the pasta is cut, you can cook it immediately in boiling salted water for about 2-5 minutes until al dente. Alternatively, you can let it dry for a few hours or overnight before cooking.\\n12. Serve the cooked pasta with your favorite sauce, such as marinara, carbonara, or pesto.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere's a step-by-step guide to making Italian pasta:\\n\\n1. On a clean surface or in a large mixing bowl, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, beat the eggs and gradually incorporate the flour from the sides of the well. Continue mixing until a dough starts to form.\\n4. Once the dough becomes too stiff to mix with a fork, use your hands to knead it. If the dough is too dry, add a little water, one tablespoon at a time, until it comes together. If it's too sticky, add a little flour.\\n5. Knead the dough for about 5-10 minutes until it becomes smooth and elastic.\\n6. Shape the dough into a ball and cover it with a clean kitchen towel. Let it rest for at least 30 minutes to allow the gluten to relax.\\n7. After resting, divide the dough into smaller portions. Flatten each portion with a rolling pin or a pasta machine until it reaches the desired thickness. You can make it as thin or thick as you prefer.\\n8. Once the dough is rolled out, you can cut it into various pasta shapes like fettuccine, spaghetti, or lasagna sheets.\\n9. If making long pasta like spaghetti or fettuccine, dust the dough with flour and roll it up loosely. Use a sharp knife to cut it into thin strips.\\n10. If making lasagna sheets, cut the rolled-out dough into rectangular pieces of the desired size.\\n11. Once the pasta is cut, you can cook it immediately in boiling salted water for about 2-5 minutes until al dente. Alternatively, you can let it dry for a few hours or overnight before cooking.\\n12. Serve the cooked pasta with your favorite sauce, such as marinara, carbonara, or pesto.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere's a step-by-step guide to making Italian pasta:\\n\\n1. On a clean surface or in a large mixing bowl, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, beat the eggs and gradually incorporate the flour from the sides of the well. Continue mixing until a dough starts to form.\\n4. Once the dough becomes too stiff to mix with a fork, use your hands to knead it. If the dough is too dry, add a little water, one tablespoon at a time, until it comes together. If it's too sticky, add a little flour.\\n5. Knead the dough for about 5-10 minutes until it becomes smooth and elastic.\\n6. Shape the dough into a ball and cover it with a clean kitchen towel. Let it rest for at least 30 minutes to allow the gluten to relax.\\n7. After resting, divide the dough into smaller portions. Flatten each portion with a rolling pin or a pasta machine until it reaches the desired thickness. You can make it as thin or thick as you prefer.\\n8. Once the dough is rolled out, you can cut it into various pasta shapes like fettuccine, spaghetti, or lasagna sheets.\\n9. If making long pasta like spaghetti or fettuccine, dust the dough with flour and roll it up loosely. Use a sharp knife to cut it into thin strips.\\n10. If making lasagna sheets, cut the rolled-out dough into rectangular pieces of the desired size.\\n11. Once the pasta is cut, you can cook it immediately in boiling salted water for about 2-5 minutes until al dente. Alternatively, you can let it dry for a few hours or overnight before cooking.\\n12. Serve the cooked pasta with your favorite sauce, such as marinara, carbonara, or pesto.\\n\\nEnjoy your homemade Italian pasta!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.5 Caching\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_debug(True)\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple recipe for making soju at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup of rice\n",
      "- 1 cup of nuruk (Korean fermentation starter)\n",
      "- 8 cups of water\n",
      "- 1 tablespoon of yeast\n",
      "- 1 cup of sugar (optional, for sweetening)\n",
      "\n",
      "Instructions:\n",
      "1. Rinse the rice thoroughly until the water runs clear.\n",
      "2. Soak the rice in water for about 1 hour, then drain.\n",
      "3. Steam the rice until it becomes soft and fully cooked.\n",
      "4. Let the cooked rice cool down to room temperature.\n",
      "5. In a large container, combine the cooled rice, nuruk, and water. Mix well.\n",
      "6. Cover the container with a clean cloth and let it ferment for about 7-10 days at room temperature.\n",
      "7. After the fermentation period, strain the mixture to remove any solids.\n",
      "8. Dissolve the yeast in a small amount of warm water and add it to the strained liquid.\n",
      "9. Cover the container again and let it ferment for an additional 2-3 days.\n",
      "10. After the second fermentation, strain the liquid again to remove any remaining solids.\n",
      "11. At this point, you can add sugar if you prefer a sweeter taste. Stir until the sugar is fully dissolved.\n",
      "12. Transfer the soju into clean bottles or jars and refrigerate for a few hours before serving.\n",
      "13. Soju is typically enjoyed chilled, either straight or mixed with other beverages or fruit juices.\n",
      "\n",
      "Please note that making soju at home may not produce the same flavor and alcohol content as commercially produced soju.Here is a basic recipe for making bread:\n",
      "\n",
      "Ingredients:\n",
      "- 4 cups all-purpose flour\n",
      "- 2 teaspoons active dry yeast\n",
      "- 2 teaspoons salt\n",
      "- 2 tablespoons sugar\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 ½ cups warm water (around 110°F/43°C)\n",
      "\n",
      "Instructions:\n",
      "1. In a large mixing bowl, combine the warm water and sugar. Stir until the sugar is dissolved.\n",
      "2. Sprinkle the yeast over the water and let it sit for about 5 minutes until it becomes foamy.\n",
      "3. Add the flour, salt, and vegetable oil to the bowl. Mix everything together until a dough forms.\n",
      "4. Transfer the dough onto a floured surface and knead it for about 10 minutes until it becomes smooth and elastic. You can also use a stand mixer with a dough hook attachment for this step.\n",
      "5. Place the dough in a greased bowl and cover it with a clean kitchen towel. Let it rise in a warm place for about 1-2 hours or until it doubles in size.\n",
      "6. Once the dough has risen, punch it down to release any air bubbles. Shape it into a loaf by rolling it tightly and tucking the ends underneath.\n",
      "7. Place the shaped dough into a greased loaf pan and cover it again with the kitchen towel. Let it rise for another 30-45 minutes.\n",
      "8. Preheat your oven to 375°F (190°C).\n",
      "9. Bake the bread in the preheated oven for about 30-35 minutes or until it turns golden brown and sounds hollow when tapped on the bottom.\n",
      "10. Remove the bread from the oven and let it cool in the pan for a few minutes. Then transfer it to a wire rack to cool completely before slicing.\n",
      "\n",
      "Note: This is a basic bread recipe, and you can customize it by adding ingredients like herbs, cheese, or seeds to the dough for different flavors.Here is a simple recipe for making soju at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup of rice\n",
      "- 1 cup of nuruk (Korean fermentation starter)\n",
      "- 8 cups of water\n",
      "- 1 tablespoon of yeast\n",
      "- 1 cup of sugar (optional, for sweetening)\n",
      "\n",
      "Instructions:\n",
      "1. Rinse the rice thoroughly until the water runs clear.\n",
      "2. Soak the rice in water for about 1 hour, then drain.\n",
      "3. Steam the rice until it becomes soft and fully cooked.\n",
      "4. Let the cooked rice cool down to room temperature.\n",
      "5. In a large container, combine the cooled rice, nuruk, and water. Mix well.\n",
      "6. Cover the container with a clean cloth and let it ferment for about 7-10 days at room temperature.\n",
      "7. After the fermentation period, strain the mixture to remove any solids.\n",
      "8. Dissolve the yeast in a small amount of warm water and add it to the strained liquid.\n",
      "9. Cover the container again and let it ferment for an additional 2-3 days.\n",
      "10. After the second fermentation, strain the liquid again to remove any remaining solids.\n",
      "11. At this point, you can add sugar if you prefer a sweeter taste. Stir until the sugar is fully dissolved.\n",
      "12. Transfer the soju into clean bottles or jars and refrigerate for a few hours before serving.\n",
      "13. Soju is typically enjoyed chilled, either straight or mixed with other beverages or fruit juices.\n",
      "\n",
      "Please note that making soju at home may not produce the same flavor and alcohol content as commercially produced soju. Here is a basic recipe for making bread:\n",
      "\n",
      "Ingredients:\n",
      "- 4 cups all-purpose flour\n",
      "- 2 teaspoons active dry yeast\n",
      "- 2 teaspoons salt\n",
      "- 2 tablespoons sugar\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 ½ cups warm water (around 110°F/43°C)\n",
      "\n",
      "Instructions:\n",
      "1. In a large mixing bowl, combine the warm water and sugar. Stir until the sugar is dissolved.\n",
      "2. Sprinkle the yeast over the water and let it sit for about 5 minutes until it becomes foamy.\n",
      "3. Add the flour, salt, and vegetable oil to the bowl. Mix everything together until a dough forms.\n",
      "4. Transfer the dough onto a floured surface and knead it for about 10 minutes until it becomes smooth and elastic. You can also use a stand mixer with a dough hook attachment for this step.\n",
      "5. Place the dough in a greased bowl and cover it with a clean kitchen towel. Let it rise in a warm place for about 1-2 hours or until it doubles in size.\n",
      "6. Once the dough has risen, punch it down to release any air bubbles. Shape it into a loaf by rolling it tightly and tucking the ends underneath.\n",
      "7. Place the shaped dough into a greased loaf pan and cover it again with the kitchen towel. Let it rise for another 30-45 minutes.\n",
      "8. Preheat your oven to 375°F (190°C).\n",
      "9. Bake the bread in the preheated oven for about 30-35 minutes or until it turns golden brown and sounds hollow when tapped on the bottom.\n",
      "10. Remove the bread from the oven and let it cool in the pan for a few minutes. Then transfer it to a wire rack to cool completely before slicing.\n",
      "\n",
      "Note: This is a basic bread recipe, and you can customize it by adding ingredients like herbs, cheese, or seeds to the dough for different flavors. \n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlagusrb/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/langchain/llms/openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/rlagusrb/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4.6 Serialization\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key, \n",
    "                  temperature=0.1, \n",
    "                  streaming=True, \n",
    "                  callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)\n",
    "\n",
    "chat2 = OpenAI(openai_api_key=openai_api_key, \n",
    "               temperature=0.1, \n",
    "               max_tokens=450, \n",
    "               model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "chat2.save(\"model.json\")\n",
    "\n",
    "chat3 = load_llm(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.0 Conversation Buffer Memory\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1 Conversation Buffer Window Memory\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"inputs\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "add_message(5, 5) # 1번 메시지는 삭제!\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Conversation Summary Memory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"inputs\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Conversation Summary Buffer Memory\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"inputs\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Conversation KG Memory\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"inputs\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Nico\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant and bustling city. How can I assist you today, Nico?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.5 Memory on LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.6 Chat Based Memory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 LCEL Based Memory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"My name is nico\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Part 1, Chapter 1\\n\\nPart One', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='1', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Inside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Outside, even through the shut window-pane, the world looked cold. Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere. The blackmoustachio'd face gazed down from every\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"The blackmoustachio'd face gazed down from every commanding corner. There was one on the house-front immediately opposite. BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"YOU, the caption said, while the dark eyes looked deep into Winston's own. Down at streetlevel another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='in the wind, alternately covering and uncovering the single word INGSOC. In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"for an instant like a bluebottle, and darted away again with a curving flight. It was the police patrol, snooping into people's windows. The patrols did not matter, however. Only the Thought Police\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='did not matter, however. Only the Thought Police mattered.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"Behind Winston's back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan. The telescreen received and transmitted simultaneously.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='received and transmitted simultaneously. Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard. There was of course no way of knowing whether you were being watched at any', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of knowing whether you were being watched at any given moment. How often, or on what system, the Thought Police plugged in on any individual wire was guesswork. It was even conceivable that they', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='was guesswork. It was even conceivable that they watched everybody all the time. But at any rate they could plug in your wire whenever they wanted to. You had to live -- did live, from habit that', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='to. You had to live -- did live, from habit that became instinct -- in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Winston kept his back turned to the telescreen. It was safer, though, as he well knew, even a back can be revealing. A kilometre away the Ministry of Truth, his place of work, towered vast and white', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Truth, his place of work, towered vast and white above the grimy landscape. This, he thought with a sort of vague distaste -- this was London, chief city of Airstrip One, itself the third most', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='chief city of Airstrip One, itself the third most populous of the provinces of Oceania. He tried to squeeze out some childhood memory that should tell him whether London had always been quite like', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='him whether London had always been quite like this. Were there always these vistas of rotting nineteenth-century houses, their sides shored up with baulks of timber, their windows patched with', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='with baulks of timber, their windows patched with cardboard and their roofs with corrugated iron, their crazy garden walls sagging in all directions? And the bombed sites where the plaster dust', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='And the bombed sites where the plaster dust swirled in the air and the willow-herb straggled over the heaps of rubble; and the places where the bombs had cleared a larger patch and there had sprung', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='had cleared a larger patch and there had sprung up sordid colonies of wooden dwellings like chicken-houses? But it was no use, he could not remember: nothing remained of his childhood except a series', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='nothing remained of his childhood except a series of bright-lit tableaux occurring against no background and mostly unintelligible.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The Ministry of Truth -- Minitrue, in Newspeak -- was startlingly different from any other object in sight. It was an enormous pyramidal structure of glittering white concrete, soaring up, terrace', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of glittering white concrete, soaring up, terrace after terrace, 300 metres into the air. From where Winston stood it was just possible to read, picked out on its white face in elegant lettering, the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='out on its white face in elegant lettering, the three slogans of the Party:', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='WAR IS PEACE\\n\\nFREEDOM IS SLAVERY\\n\\nIGNORANCE IS STRENGTH', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The Ministry of Truth contained, it was said, three thousand rooms above ground level, and corresponding ramifications below. Scattered about London there were just three other buildings of similar', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='there were just three other buildings of similar appearance and size. So completely did they dwarf the surrounding architecture that from the roof of Victory Mansions you could see all four of them', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Victory Mansions you could see all four of them simultaneously. They were the homes of the four Ministries between which the entire apparatus of government was divided. The Ministry of Truth, which', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='was divided. The Ministry of Truth, which concerned itself with news, entertainment, education, and the fine arts. The Ministry of Peace, which concerned itself with war. The Ministry of Love, which', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='itself with war. The Ministry of Love, which maintained law and order. And the Ministry of Plenty, which was responsible for economic affairs. Their names, in Newspeak: Minitrue, Minipax, Miniluv,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='names, in Newspeak: Minitrue, Minipax, Miniluv, and Miniplenty.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The Ministry of Love was the really frightening one. There were no windows in it at all. Winston had never been inside the Ministry of Love, nor within half a kilometre of it. It was a place', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='nor within half a kilometre of it. It was a place impossible to enter except on official business, and then only by penetrating through a maze of barbed-wire entanglements, steel doors, and hidden', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='entanglements, steel doors, and hidden machine-gun nests. Even the streets leading up to its outer barriers were roamed by gorilla-faced guards in black uniforms, armed with jointed truncheons.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Winston turned round abruptly. He had set his features into the expression of quiet optimism which it was advisable to wear when facing the telescreen. He crossed the room into the tiny kitchen. By', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='He crossed the room into the tiny kitchen. By leaving the Ministry at this time of day he had sacrificed his lunch in the canteen, and he was aware that there was no food in the kitchen except a hunk', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"there was no food in the kitchen except a hunk of dark-coloured bread which had got to be saved for tomorrow's breakfast. He took down from the shelf a bottle of colourless liquid with a plain white\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='a bottle of colourless liquid with a plain white label marked VICTORY GIN. It gave off a sickly, oily smell, as of Chinese ricespirit. Winston poured out nearly a teacupful, nerved himself for a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='out nearly a teacupful, nerved himself for a shock, and gulped it down like a dose of medicine.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Instantly his face turned scarlet and the water ran out of his eyes. The stuff was like nitric acid, and moreover, in swallowing it one had the sensation of being hit on the back of the head with a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of being hit on the back of the head with a rubber club. The next moment, however, the burning in his belly died down and the world began to look more cheerful. He took a cigarette from a crumpled', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='cheerful. He took a cigarette from a crumpled packet marked VICTORY CIGARETTES and incautiously held it upright, whereupon the tobacco fell out on to the floor. With the next he was more successful.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the floor. With the next he was more successful. He went back to the living-room and sat down at a small table that stood to the left of the telescreen. From the table drawer he took out a penholder,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='From the table drawer he took out a penholder, a bottle of ink, and a thick, quarto-sized blank book with a red back and a marbled cover.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='For some reason the telescreen in the living-room was in an unusual position. Instead of being placed, as was normal, in the end wall, where it could command the whole room, it was in the longer', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='command the whole room, it was in the longer wall, opposite the window. To one side of it there was a shallow alcove in which Winston was now sitting, and which, when the flats were built, had', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='and which, when the flats were built, had probably been intended to hold bookshelves. By sitting in the alcove, and keeping well back, Winston was able to remain outside the range of the telescreen,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='to remain outside the range of the telescreen, so far as sight went. He could be heard, of course, but so long as he stayed in his present position he could not be seen. It was partly the unusual', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='he could not be seen. It was partly the unusual geography of the room that had suggested to him the thing that he was now about to do.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='But it had also been suggested by the book that he had just taken out of the drawer. It was a peculiarly beautiful book. Its smooth creamy paper, a little yellowed by age, was of a kind that had not', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='yellowed by age, was of a kind that had not been manufactured for at least forty years past. He could guess, however, that the book was much older than that. He had seen it lying in the window of a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='that. He had seen it lying in the window of a frowsy little junk-shop in a slummy quarter of the town (just what quarter he did not now remember) and had been stricken immediately by an overwhelming', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"had been stricken immediately by an overwhelming desire to possess it. Party members were supposed not to go into ordinary shops ('dealing on the free market', it was called), but the rule was not\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"market', it was called), but the rule was not strictly kept, because there were various things, such as shoelaces and razor blades, which it was impossible to get hold of in any other way. He had\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='to get hold of in any other way. He had given a quick glance up and down the street and then had slipped inside and bought the book for two dollars fifty. At the time he was not conscious of wanting', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='At the time he was not conscious of wanting it for any particular purpose. He had carried it guiltily home in his briefcase. Even with nothing written in it, it was a compromising possession.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The thing that he was about to do was to open a diary. This was not illegal (nothing was illegal, since there were no longer any laws), but if detected it was reasonably certain that it would be', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='it was reasonably certain that it would be punished by death, or at least by twenty-five years in a forced-labour camp. Winston fitted a nib into the penholder and sucked it to get the grease off.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='penholder and sucked it to get the grease off. The pen was an archaic instrument, seldom used even for signatures, and he had procured one, furtively and with some difficulty, simply because of a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='and with some difficulty, simply because of a feeling that the beautiful creamy paper deserved to be written on with a real nib instead of being scratched with an ink-pencil. Actually he was not used', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='with an ink-pencil. Actually he was not used to writing by hand. Apart from very short notes, it was usual to dictate everything into the speakwrite which was of course impossible for his present', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='which was of course impossible for his present purpose. He dipped the pen into the ink and then faltered for just a second. A tremor had gone through his bowels. To mark the paper was the decisive', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='his bowels. To mark the paper was the decisive act. In small clumsy letters he wrote:', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='April 4th, 1984.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='He sat back. A sense of complete helplessness had descended upon him. To begin with, he did not know with any certainty that this was 1984. It must be round about that date, since he was fairly sure', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='round about that date, since he was fairly sure that his age was thirty-nine, and he believed that he had been born in 1944 or 1945; but it was never possible nowadays to pin down any date within a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='possible nowadays to pin down any date within a year or two.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='For whom, it suddenly occurred to him to wonder, was he writing this diary? For the future, for the unborn. His mind hovered for a moment round the doubtful date on the page, and then fetched up with', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='date on the page, and then fetched up with a bump against the Newspeak word doublethink. For the first time the magnitude of what he had undertaken came home to him. How could you communicate with', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='came home to him. How could you communicate with the future? It was of its nature impossible. Either the future would resemble the present, in which case it would not listen to him: or it would be', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='case it would not listen to him: or it would be different from it, and his predicament would be meaningless.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='For some time he sat gazing stupidly at the paper. The telescreen had changed over to strident military music. It was curious that he seemed not merely to have lost the power of expressing himself,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='to have lost the power of expressing himself, but even to have forgotten what it was that he had originally intended to say. For weeks past he had been making ready for this moment, and it had never', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='making ready for this moment, and it had never crossed his mind that anything would be needed except courage. The actual writing would be easy. All he had to do was to transfer to paper the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='All he had to do was to transfer to paper the interminable restless monologue that had been running inside his head, literally for years. At this moment, however, even the monologue had dried up.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='moment, however, even the monologue had dried up. Moreover his varicose ulcer had begun itching unbearably. He dared not scratch it, because if he did so it always became inflamed. The seconds were', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='so it always became inflamed. The seconds were ticking by. He was conscious of nothing except the blankness of the page in front of him, the itching of the skin above his ankle, the blaring of the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of the skin above his ankle, the blaring of the music, and a slight booziness caused by the gin.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Suddenly he began writing in sheer panic, only imperfectly aware of what he was setting down. His small but childish handwriting straggled up and down the page, shedding first its capital letters and', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the page, shedding first its capital letters and finally even its full stops:', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='April 4th, 1984. Last night to the flicks. All war films. One very good one of a ship full of refugees being bombed somewhere in the Mediterranean. Audience much amused by shots of a great huge fat', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Audience much amused by shots of a great huge fat man trying to swim away with a helicopter after him, first you saw him wallowing along in the water like a porpoise, then you saw him through the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='like a porpoise, then you saw him through the helicopters gunsights, then he was full of holes and the sea round him turned pink and he sank as suddenly as though the holes had let in the water,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='as though the holes had let in the water, audience shouting with laughter when he sank. then you saw a lifeboat full of children with a helicopter hovering over it. there was a middle-aged woman', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='hovering over it. there was a middle-aged woman might have been a jewess sitting up in the bow with a little boy about three years old in her arms. little boy screaming with fright and hiding his', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='little boy screaming with fright and hiding his head between her breasts as if he was trying to burrow right into her and the woman putting her arms round him and comforting him although she was blue', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='him and comforting him although she was blue with fright herself, all the time covering him up as much as possible as if she thought her arms could keep the bullets off him. then the helicopter', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"keep the bullets off him. then the helicopter planted a 20 kilo bomb in among them terrific flash and the boat went all to matchwood. then there was a wonderful shot of a child's arm going up up up\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"a wonderful shot of a child's arm going up up up right up into the air a helicopter with a camera in its nose must have followed it up and there was a lot of applause from the party seats but a woman\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='lot of applause from the party seats but a woman down in the prole part of the house suddenly started kicking up a fuss and shouting they didnt oughter of showed it not in front of kids they didnt it', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of showed it not in front of kids they didnt it aint right not in front of kids it aint until the police turned her turned her out i dont suppose anything happened to her nobody cares what the proles', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='happened to her nobody cares what the proles say typical prole reaction they never --', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Winston stopped writing, partly because he was suffering from cramp. He did not know what had made him pour out this stream of rubbish. But the curious thing was that while he was doing so a totally', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='thing was that while he was doing so a totally different memory had clarified itself in his mind, to the point where he almost felt equal to writing it down. It was, he now realized, because of this', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='it down. It was, he now realized, because of this other incident that he had suddenly decided to come home and begin the diary today.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='It had happened that morning at the Ministry, if anything so nebulous could be said to happen.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='It was nearly eleven hundred, and in the Records Department, where Winston worked, they were dragging the chairs out of the cubicles and grouping them in the centre of the hall opposite the big', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='them in the centre of the hall opposite the big telescreen, in preparation for the Two Minutes Hate. Winston was just taking his place in one of the middle rows when two people whom he knew by sight,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='rows when two people whom he knew by sight, but had never spoken to, came unexpectedly into the room. One of them was a girl whom he often passed in the corridors. He did not know her name, but he', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the corridors. He did not know her name, but he knew that she worked in the Fiction Department. Presumably -- since he had sometimes seen her with oily hands and carrying a spanner she had some', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='oily hands and carrying a spanner she had some mechanical job on one of the novel-writing machines. She was a bold-looking girl, of about twenty-seven, with thick hair, a freckled face, and swift,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='with thick hair, a freckled face, and swift, athletic movements. A narrow scarlet sash, emblem of the Junior Anti-Sex League, was wound several times round the waist of her overalls, just tightly', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='round the waist of her overalls, just tightly enough to bring out the shapeliness of her hips. Winston had disliked her from the very first moment of seeing her. He knew the reason. It was because of', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='seeing her. He knew the reason. It was because of the atmosphere of hockey-fields and cold baths and community hikes and general clean-mindedness which she managed to carry about with her. He', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='which she managed to carry about with her. He disliked nearly all women, and especially the young and pretty ones. It was always the women, and above all the young ones, who were the most bigoted', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='all the young ones, who were the most bigoted adherents of the Party, the swallowers of slogans, the amateur spies and nosers-out of unorthodoxy. But this particular girl gave him the impression of', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='this particular girl gave him the impression of being more dangerous than most. Once when they passed in the corridor she gave him a quick sidelong glance which seemed to pierce right into him and', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='glance which seemed to pierce right into him and for a moment had filled him with black terror. The idea had even crossed his mind that she might be an agent of the Thought Police. That, it was true,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='agent of the Thought Police. That, it was true, was very unlikely. Still, he continued to feel a peculiar uneasiness, which had fear mixed up in it as well as hostility, whenever she was anywhere', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='as well as hostility, whenever she was anywhere near him.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"The other person was a man named O'Brien, a member of the Inner Party and holder of some post so important and remote that Winston had only a dim idea of its nature. A momentary hush passed over the\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"of its nature. A momentary hush passed over the group of people round the chairs as they saw the black overalls of an Inner Party member approaching. O'Brien was a large, burly man with a thick neck\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"O'Brien was a large, burly man with a thick neck and a coarse, humorous, brutal face. In spite of his formidable appearance he had a certain charm of manner. He had a trick of resettling his\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='charm of manner. He had a trick of resettling his spectacles on his nose which was curiously disarming -- in some indefinable way, curiously civilized. It was a gesture which, if anyone had still', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"It was a gesture which, if anyone had still thought in such terms, might have recalled an eighteenth-century nobleman offering his snuffbox. Winston had seen O'Brien perhaps a dozen times in almost\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"had seen O'Brien perhaps a dozen times in almost as many years. He felt deeply drawn to him, and not solely because he was intrigued by the contrast between O'Brien's urbane manner and his\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"contrast between O'Brien's urbane manner and his prize-fighter's physique. Much more it was because of a secretly held belief -- or perhaps not even a belief, merely a hope -- that O'Brien's\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"even a belief, merely a hope -- that O'Brien's political orthodoxy was not perfect. Something in his face suggested it irresistibly. And again, perhaps it was not even unorthodoxy that was written in\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='it was not even unorthodoxy that was written in his face, but simply intelligence. But at any rate he had the appearance of being a person that you could talk to if somehow you could cheat the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='you could talk to if somehow you could cheat the telescreen and get him alone. Winston had never made the smallest effort to verify this guess: indeed, there was no way of doing so. At this moment', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"there was no way of doing so. At this moment O'Brien glanced at his wrist-watch, saw that it was nearly eleven hundred, and evidently decided to stay in the Records Department until the Two Minutes\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='in the Records Department until the Two Minutes Hate was over. He took a chair in the same row as Winston, a couple of places away. A small, sandy-haired woman who worked in the next cubicle to', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='woman who worked in the next cubicle to Winston was between them. The girl with dark hair was sitting immediately behind.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"The next moment a hideous, grinding speech, as of some monstrous machine running without oil, burst from the big telescreen at the end of the room. It was a noise that set one's teeth on edge and\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"It was a noise that set one's teeth on edge and bristled the hair at the back of one's neck. The Hate had started.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='As usual, the face of Emmanuel Goldstein, the Enemy of the People, had flashed on to the screen. There were hisses here and there among the audience. The little sandy-haired woman gave a squeak of', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The little sandy-haired woman gave a squeak of mingled fear and disgust. Goldstein was the renegade and backslider who once, long ago (how long ago, nobody quite remembered), had been one of the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='nobody quite remembered), had been one of the leading figures of the Party, almost on a level with Big Brother himself, and then had engaged in counter-revolutionary activities, had been condemned to', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='activities, had been condemned to death, and had mysteriously escaped and disappeared. The programmes of the Two Minutes Hate varied from day to day, but there was none in which Goldstein was not the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"but there was none in which Goldstein was not the principal figure. He was the primal traitor, the earliest defiler of the Party's purity. All subsequent crimes against the Party, all treacheries,\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='crimes against the Party, all treacheries, acts of sabotage, heresies, deviations, sprang directly out of his teaching. Somewhere or other he was still alive and hatching his conspiracies: perhaps', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='alive and hatching his conspiracies: perhaps somewhere beyond the sea, under the protection of his foreign paymasters, perhaps even -- so it was occasionally rumoured -- in some hiding-place in', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='occasionally rumoured -- in some hiding-place in Oceania itself.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"Winston's diaphragm was constricted. He could never see the face of Goldstein without a painful mixture of emotions. It was a lean Jewish face, with a great fuzzy aureole of white hair and a small\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='a great fuzzy aureole of white hair and a small goatee beard -- a clever face, and yet somehow inherently despicable, with a kind of senile silliness in the long thin nose, near the end of which a', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='in the long thin nose, near the end of which a pair of spectacles was perched. It resembled the face of a sheep, and the voice, too, had a sheep-like quality. Goldstein was delivering his usual', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='quality. Goldstein was delivering his usual venomous attack upon the doctrines of the Party -- an attack so exaggerated and perverse that a child should have been able to see through it, and yet just', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='have been able to see through it, and yet just plausible enough to fill one with an alarmed feeling that other people, less level-headed than oneself, might be taken in by it. He was abusing Big', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='might be taken in by it. He was abusing Big Brother, he was denouncing the dictatorship of the Party, he was demanding the immediate conclusion of peace with Eurasia, he was advocating freedom of', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='peace with Eurasia, he was advocating freedom of speech, freedom of the Press, freedom of assembly, freedom of thought, he was crying hysterically that the revolution had been betrayed -- and all', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='that the revolution had been betrayed -- and all this in rapid polysyllabic speech which was a sort of parody of the habitual style of the orators of the Party, and even contained Newspeak words:', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of the Party, and even contained Newspeak words: more Newspeak words, indeed, than any Party member would normally use in real life. And all the while, lest one should be in any doubt as to the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"while, lest one should be in any doubt as to the reality which Goldstein's specious claptrap covered, behind his head on the telescreen there marched the endless columns of the Eurasian army -- row\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the endless columns of the Eurasian army -- row after row of solid-looking men with expressionless Asiatic faces, who swam up to the surface of the screen and vanished, to be replaced by others', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"the screen and vanished, to be replaced by others exactly similar. The dull rhythmic tramp of the soldiers' boots formed the background to Goldstein's bleating voice.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Before the Hate had proceeded for thirty seconds, uncontrollable exclamations of rage were breaking out from half the people in the room. The self-satisfied sheep-like face on the screen, and the', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='sheep-like face on the screen, and the terrifying power of the Eurasian army behind it, were too much to be borne: besides, the sight or even the thought of Goldstein produced fear and anger', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the thought of Goldstein produced fear and anger automatically. He was an object of hatred more constant than either Eurasia or Eastasia, since when Oceania was at war with one of these Powers it was', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='was at war with one of these Powers it was generally at peace with the other. But what was strange was that although Goldstein was hated and despised by everybody, although every day and a thousand', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='by everybody, although every day and a thousand times a day, on platforms, on the telescreen, in newspapers, in books, his theories were refuted, smashed, ridiculed, held up to the general gaze for', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='ridiculed, held up to the general gaze for the pitiful rubbish that they were in spite of all this, his influence never seemed to grow less. Always there were fresh dupes waiting to be seduced by', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='there were fresh dupes waiting to be seduced by him. A day never passed when spies and saboteurs acting under his directions were not unmasked by the Thought Police. He was the commander of a vast', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Thought Police. He was the commander of a vast shadowy army, an underground network of conspirators dedicated to the overthrow of the State. The Brotherhood, its name was supposed to be. There were', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='its name was supposed to be. There were also whispered stories of a terrible book, a compendium of all the heresies, of which Goldstein was the author and which circulated clandestinely here and', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='and which circulated clandestinely here and there. It was a book without a title. People referred to it, if at all, simply as the book. But one knew of such things only through vague rumours. Neither', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='such things only through vague rumours. Neither the Brotherhood nor the book was a subject that any ordinary Party member would mention if there was a way of avoiding it.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='In its second minute the Hate rose to a frenzy. People were leaping up and down in their places and shouting at the tops of their voices in an effort to drown the maddening bleating voice that came', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='to drown the maddening bleating voice that came from the screen. The little sandy-haired woman had turned bright pink, and her mouth was opening and shutting like that of a landed fish. Even', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"and shutting like that of a landed fish. Even O'Brien's heavy face was flushed. He was sitting very straight in his chair, his powerful chest swelling and quivering as though he were standing up to\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"and quivering as though he were standing up to the assault of a wave. The dark-haired girl behind Winston had begun crying out 'Swine! Swine! Swine!' and suddenly she picked up a heavy Newspeak\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"and suddenly she picked up a heavy Newspeak dictionary and flung it at the screen. It struck Goldstein's nose and bounced off; the voice continued inexorably. In a lucid moment Winston found that he\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='In a lucid moment Winston found that he was shouting with the others and kicking his heel violently against the rung of his chair. The horrible thing about the Two Minutes Hate was not that one was', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='about the Two Minutes Hate was not that one was obliged to act a part, but, on the contrary, that it was impossible to avoid joining in. Within thirty seconds any pretence was always unnecessary. A', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='seconds any pretence was always unnecessary. A hideous ecstasy of fear and vindictiveness, a desire to kill, to torture, to smash faces in with a sledge-hammer, seemed to flow through the whole group', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"seemed to flow through the whole group of people like an electric current, turning one even against one's will into a grimacing, screaming lunatic. And yet the rage that one felt was an abstract,\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"And yet the rage that one felt was an abstract, undirected emotion which could be switched from one object to another like the flame of a blowlamp. Thus, at one moment Winston's hatred was not turned\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"at one moment Winston's hatred was not turned against Goldstein at all, but, on the contrary, against Big Brother, the Party, and the Thought Police; and at such moments his heart went out to the\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='and at such moments his heart went out to the lonely, derided heretic on the screen, sole guardian of truth and sanity in a world of lies. And yet the very next instant he was at one with the people', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='very next instant he was at one with the people about him, and all that was said of Goldstein seemed to him to be true. At those moments his secret loathing of Big Brother changed into adoration, and', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of Big Brother changed into adoration, and Big Brother seemed to tower up, an invincible, fearless protector, standing like a rock against the hordes of Asia, and Goldstein, in spite of his', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='hordes of Asia, and Goldstein, in spite of his isolation, his helplessness, and the doubt that hung about his very existence, seemed like some sinister enchanter, capable by the mere power of his', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='enchanter, capable by the mere power of his voice of wrecking the structure of civilization.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"It was even possible, at moments, to switch one's hatred this way or that by a voluntary act. Suddenly, by the sort of violent effort with which one wrenches one's head away from the pillow in a\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"one wrenches one's head away from the pillow in a nightmare, Winston succeeded in transferring his hatred from the face on the screen to the dark-haired girl behind him. Vivid, beautiful\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the dark-haired girl behind him. Vivid, beautiful hallucinations flashed through his mind. He would flog her to death with a rubber truncheon. He would tie her naked to a stake and shoot her full of', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='tie her naked to a stake and shoot her full of arrows like Saint Sebastian. He would ravish her and cut her throat at the moment of climax. Better than before, moreover, he realized why it was that', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='before, moreover, he realized why it was that he hated her. He hated her because she was young and pretty and sexless, because he wanted to go to bed with her and would never do so, because round her', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='with her and would never do so, because round her sweet supple waist, which seemed to ask you to encircle it with your arm, there was only the odious scarlet sash, aggressive symbol of chastity.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"The Hate rose to its climax. The voice of Goldstein had become an actual sheep's bleat, and for an instant the face changed into that of a sheep. Then the sheep-face melted into the figure of a\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Then the sheep-face melted into the figure of a Eurasian soldier who seemed to be advancing, huge and terrible, his sub-machine gun roaring, and seeming to spring out of the surface of the screen, so', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='to spring out of the surface of the screen, so that some of the people in the front row actually flinched backwards in their seats. But in the same moment, drawing a deep sigh of relief from', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"same moment, drawing a deep sigh of relief from everybody, the hostile figure melted into the face of Big Brother, black-haired, black-moustachio'd, full of power and mysterious calm, and so vast\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='full of power and mysterious calm, and so vast that it almost filled up the screen. Nobody heard what Big Brother was saying. It was merely a few words of encouragement, the sort of words that are', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='of encouragement, the sort of words that are uttered in the din of battle, not distinguishable individually but restoring confidence by the fact of being spoken. Then the face of Big Brother faded', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='being spoken. Then the face of Big Brother faded away again, and instead the three slogans of the Party stood out in bold capitals:', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='WAR IS PEACE\\n\\nFREEDOM IS SLAVERY\\n\\nIGNORANCE IS STRENGTH', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"But the face of Big Brother seemed to persist for several seconds on the screen, as though the impact that it had made on everyone's eyeballs was too vivid to wear off immediately. The little\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"was too vivid to wear off immediately. The little sandyhaired woman had flung herself forward over the back of the chair in front of her. With a tremulous murmur that sounded like 'My Saviour!' she\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"murmur that sounded like 'My Saviour!' she extended her arms towards the screen. Then she buried her face in her hands. It was apparent that she was uttering a prayer.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"At this moment the entire group of people broke into a deep, slow, rhythmical chant of 'B-B! ...B-B!' -- over and over again, very slowly, with a long pause between the first 'B' and the second-a\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"long pause between the first 'B' and the second-a heavy, murmurous sound, somehow curiously savage, in the background of which one seemed to hear the stamp of naked feet and the throbbing of\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='hear the stamp of naked feet and the throbbing of tom-toms. For perhaps as much as thirty seconds they kept it up. It was a refrain that was often heard in moments of overwhelming emotion. Partly it', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='in moments of overwhelming emotion. Partly it was a sort of hymn to the wisdom and majesty of Big Brother, but still more it was an act of self-hypnosis, a deliberate drowning of consciousness by', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"a deliberate drowning of consciousness by means of rhythmic noise. Winston's entrails seemed to grow cold. In the Two Minutes Hate he could not help sharing in the general delirium, but this\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"help sharing in the general delirium, but this sub-human chanting of 'B-B! ...B-B!' always filled him with horror. Of course he chanted with the rest: it was impossible to do otherwise. To dissemble\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='it was impossible to do otherwise. To dissemble your feelings, to control your face, to do what everyone else was doing, was an instinctive reaction. But there was a space of a couple of seconds', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='But there was a space of a couple of seconds during which the expression of his eyes might conceivably have betrayed him. And it was exactly at this moment that the significant thing happened -- if,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='moment that the significant thing happened -- if, indeed, it did happen.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"Momentarily he caught O'Brien's eye. O'Brien had stood up. He had taken off his spectacles and was in the act of resettling them on his nose with his characteristic gesture. But there was a fraction\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"characteristic gesture. But there was a fraction of a second when their eyes met, and for as long as it took to happen Winston knew-yes, he knew!-that O'Brien was thinking the same thing as himself.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"O'Brien was thinking the same thing as himself. An unmistakable message had passed. It was as though their two minds had opened and the thoughts were flowing from one into the other through their\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"flowing from one into the other through their eyes. 'I am with you,' O'Brien seemed to be saying to him. 'I know precisely what you are feeling. I know all about your contempt, your hatred, your\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"I know all about your contempt, your hatred, your disgust. But don't worry, I am on your side!' And then the flash of intelligence was gone, and O'Brien's face was as inscrutable as everybody else's.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='That was all, and he was already uncertain whether it had happened. Such incidents never had any sequel. All that they did was to keep alive in him the belief, or hope, that others besides himself', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the belief, or hope, that others besides himself were the enemies of the Party. Perhaps the rumours of vast underground conspiracies were true after all -- perhaps the Brotherhood really existed! It', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='all -- perhaps the Brotherhood really existed! It was impossible, in spite of the endless arrests and confessions and executions, to be sure that the Brotherhood was not simply a myth. Some days he', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Brotherhood was not simply a myth. Some days he believed in it, some days not. There was no evidence, only fleeting glimpses that might mean anything or nothing: snatches of overheard conversation,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='or nothing: snatches of overheard conversation, faint scribbles on lavatory walls -- once, even, when two strangers met, a small movement of the hand which had looked as though it might be a signal', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"which had looked as though it might be a signal of recognition. It was all guesswork: very likely he had imagined everything. He had gone back to his cubicle without looking at O'Brien again. The\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"his cubicle without looking at O'Brien again. The idea of following up their momentary contact hardly crossed his mind. It would have been inconceivably dangerous even if he had known how to set\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='dangerous even if he had known how to set about doing it. For a second, two seconds, they had exchanged an equivocal glance, and that was the end of the story. But even that was a memorable event, in', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='story. But even that was a memorable event, in the locked loneliness in which one had to live.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Winston roused himself and sat up straighter. He let out a belch. The gin was rising from his stomach.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='His eyes re-focused on the page. He discovered that while he sat helplessly musing he had also been writing, as though by automatic action. And it was no longer the same cramped, awkward handwriting', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='no longer the same cramped, awkward handwriting as before. His pen had slid voluptuously over the smooth paper, printing in large neat capitals -', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='DOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nover and over again, filling half a page.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='He could not help feeling a twinge of panic. It was absurd, since the writing of those particular words was not more dangerous than the initial act of opening the diary, but for a moment he was', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='act of opening the diary, but for a moment he was tempted to tear out the spoiled pages and abandon the enterprise altogether.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='He did not do so, however, because he knew that it was useless. Whether he wrote DOWN WITH BIG BROTHER, or whether he refrained from writing it, made no difference. Whether he went on with the diary,', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='no difference. Whether he went on with the diary, or whether he did not go on with it, made no difference. The Thought Police would get him just the same. He had committed -- would still have', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='the same. He had committed -- would still have committed, even if he had never set pen to paper -- the essential crime that contained all others in itself. Thoughtcrime, they called it. Thoughtcrime', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Thoughtcrime, they called it. Thoughtcrime was not a thing that could be concealed for ever. You might dodge successfully for a while, even for years, but sooner or later they were bound to get you.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='It was always at night -- the arrests invariably happened at night. The sudden jerk out of sleep, the rough hand shaking your shoulder, the lights glaring in your eyes, the ring of hard faces round', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='in your eyes, the ring of hard faces round the bed. In the vast majority of cases there was no trial, no report of the arrest. People simply disappeared, always during the night. Your name was', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='always during the night. Your name was removed from the registers, every record of everything you had ever done was wiped out, your one-time existence was denied and then forgotten. You were', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='existence was denied and then forgotten. You were abolished, annihilated: vaporized was the usual word.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='For a moment he was seized by a kind of hysteria. He began writing in a hurried untidy scrawl:', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"theyll shoot me i don't care theyll shoot me in the back of the neck i dont care down with big brother they always shoot you in the back of the neck i dont care down with big brother --\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='He sat back in his chair, slightly ashamed of himself, and laid down the pen. The next moment he started violently. There was a knocking at the door.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Already! He sat as still as a mouse, in the futile hope that whoever it was might go away after a single attempt. But no, the knocking was repeated. The worst thing of all would be to delay. His', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='The worst thing of all would be to delay. His heart was thumping like a drum, but his face, from long habit, was probably expressionless. He got up and moved heavily towards the door.', metadata={'source': './files/chapter_one.txt'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1 Data Loaders and Splitters\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredFileLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "loader2 = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splitter2 = CharacterTextSplitter(separator=\"\\n\", \n",
    "                                  chunk_size=600,\n",
    "                                  chunk_overlap=100)\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Tiktoken\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredFileLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "loader2 = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "loader.load()\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(separator=\"\\n\", \n",
    "                                  chunk_size=600,\n",
    "                                  chunk_overlap=100)\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1536\n"
     ]
    }
   ],
   "source": [
    "# 6.4 Vector Store\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "embedder.embed_query(\"Hi\")\n",
    "\n",
    "vector = embedder.embed_documents(['hi', 'how', 'are', 'you'])\n",
    "print(len(vector), len(vector[0]))\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "chache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, chache_dir\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "vectorstore_cache = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "vectorstore.similarity_search(\"where does winston live\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the new context provided, Winston Smith lives in Victory Mansions.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.6 RetrievalQA\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "embedder.embed_query(\"Hi\")\n",
    "\n",
    "vector = embedder.embed_documents(['hi', 'how', 'are', 'you'])\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "chache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, chache_dir\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "vectorstore_cache = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "vectorstore.similarity_search(\"where does winston live\")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\", # 그 외에도 refine, map_reduce, map_rerank 존재\n",
    "    retriever=vectorstore.as_retriever(),   \n",
    ")\n",
    "\n",
    "chain.run(\"Where does Winston live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is described as having glass doors at the entrance, which allow gritty dust to enter along with people. The hallway of Victory Mansions has a smell of boiled cabbage and old rag mats. On one end of the hallway, there is a large colored poster depicting the face of a man in his forties with a black mustache. The building has seven floors, and the flat where Winston lives is on the seventh floor. The flat is accessed by stairs since the lift is rarely working. The building is not well-maintained, with rotting houses and patched windows. From the roof of Victory Mansions, one can see the other three buildings that house the Ministries of Truth, Peace, and Love.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.8 Stuff LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 Stuff LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Three ministries are mentioned in the given text: the Ministry of Love, the Ministry of Plenty, and the Ministry of Truth.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.9 Map Reduce LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Introduction\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Hello World\")\n",
    "\n",
    "st.subheader(\"Welcome to Streamlit!\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    #### I love it!\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Magic\n",
    "\n",
    "import streamlit as st\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "st.title(\"Hello World\")\n",
    "\n",
    "st.subheader(\"Welcome to Streamlit!\")\n",
    "\n",
    "st.write([1, 2, 3, 4])\n",
    "st.write(PromptTemplate) # class 출력\n",
    "\n",
    "p = PromptTemplate.from_template(\"xxxx\")\n",
    "st.write(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Data Flow\n",
    "\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime(\"%H:%M:%S\")\n",
    "\n",
    "st.title(today)\n",
    "\n",
    "model = st.selectbox(\n",
    "    \"Choose your model\",\n",
    "    (\n",
    "        \"GPT-3\",\n",
    "        \"GPT-4\"\n",
    "    ),\n",
    ")\n",
    "if model == 'GPT-3':\n",
    "    st.write(\"cheap\")\n",
    "else:\n",
    "    st.write(\"expensive\")\n",
    "\n",
    "name = st.text_input(\"What is your name?\")\n",
    "st.write(name)\n",
    "\n",
    "value = st.slider(\"temperature\", min_value=0.1, max_value=1.0)\n",
    "st.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Multi Page\n",
    "\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "\n",
    "st.title(\"title\")\n",
    "\n",
    "st.sidebar.text_input(\"what is name?\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.title(\"sidebar title\")\n",
    "\n",
    "tab_one, tab_two, tab_three = st.tabs([\"A\", \"B\", \"C\"])\n",
    "\n",
    "with tab_one:\n",
    "    st.write(\"a\")\n",
    "\n",
    "with tab_two:\n",
    "    st.write(\"b\")\n",
    "    \n",
    "with tab_three:\n",
    "    st.write(\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Chat Messages\n",
    "\n",
    "import time\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.write(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    send_message(\n",
    "        message[\"message\"],\n",
    "        message[\"role\"],\n",
    "        save=False,\n",
    "    )\n",
    "\n",
    "\n",
    "message = st.chat_input(\"Send a message to the ai \")\n",
    "\n",
    "if message:\n",
    "    send_message(message, \"human\")\n",
    "    time.sleep(2)\n",
    "    send_message(f\"You said: {message}\", \"ai\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.write(st.session_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6 Uploading Documents\n",
    "\n",
    "import time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "file = st.file_uploader(\n",
    "    \"Upload a .txt .pdf or .docx file\",\n",
    "    type=[\"pdf\", \"txt\", \"docx\"],\n",
    ")\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    s = retriever.invoke(\"winston\")\n",
    "    s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7 Chat History\n",
    "\n",
    "import time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.8 Chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        response = chain.invoke(message)\n",
    "        send_message(response.content, \"ai\")\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.9 Streaming\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        with st.chat_message(\"ai\"):\n",
    "            response = chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.10 Recap\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        with st.chat_message(\"ai\"):\n",
    "            chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlagusrb/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Potato is a starchy vegetable that is grown underground in rows in a shallow, well-drained soil. It is a tuber, which means it is an underground, fleshy part of the plant that stores nutrients. Potatoes are a popular food source and are used in many different dishes, including salads, soups, stews, and baked dishes. They are also used as a source of starch for making paper and other products. Potatoes are high in carbohydrates and are a good source of fiber, vitamin C, and potassium.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.1 HuggingFaceHub\n",
    "\n",
    "import os\n",
    "# .env 파일 쓰려면 dotenv 사용하기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ.get('HUGGINGFACE_API_TOKEN')\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", # 모델명\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 250, # token 얼마나 생성할지 정함\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"potato\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' tomato. It tastes like butter and cream but it has no flavor. It smells like a tomato tomato. It\\'s not sweet, because there\\'s no flavor in it so it\\'s basically a tomato jam. I think a tomato jam may be a little like something hot, like if you want ice cream, but not as good as a tomato jam. But if you want to drink ice cream, you can get a tomato jam from Starbucks (so they have a variety), at the Wal-Mart if you are an American. The only sugar that\\'s added to the tomato jam may only be sugar. Some people think that there\\'s some kind of \"reduce sugar\" ingredient in the tomato jam—it\\'s the sugar that gets absorbed into the tomato'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.2 HuggingFacePipeline\n",
    "\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150},\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/rlagusrb/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that defines words. Define this word: \u001b[39m\u001b[38;5;132;01m{word}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./falcon.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     16\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtomato\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/langchain/llms/gpt4all.py:131\u001b[0m, in \u001b[0;36mGPT4All.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate that the python package exists in the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4All \u001b[38;5;28;01mas\u001b[39;00m GPT4AllModel\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import gpt4all python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install gpt4all`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/gpt4all/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embed4All, GPT4All  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyllmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLModel  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/gpt4all/gpt4all.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncompleteRead, ProtocolError\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyllmodel\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TODO: move to config\u001b[39;00m\n\u001b[1;32m     21\u001b[0m DEFAULT_MODEL_DIRECTORY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mhome()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4all\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/gpt4all/pyllmodel.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m         lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(\u001b[38;5;28mstr\u001b[39m(MODEL_LIB_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmodel.dll\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[0;32m---> 42\u001b[0m llmodel \u001b[38;5;241m=\u001b[39m \u001b[43mload_llmodel_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLModelError\u001b[39;00m(ctypes\u001b[38;5;241m.\u001b[39mStructure):\n\u001b[1;32m     46\u001b[0m     _fields_ \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctypes\u001b[38;5;241m.\u001b[39mc_char_p), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctypes\u001b[38;5;241m.\u001b[39mc_int32)]\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/gpt4all/pyllmodel.py:32\u001b[0m, in \u001b[0;36mload_llmodel_library\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m ext \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdylib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinux\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdll\u001b[39m\u001b[38;5;124m\"\u001b[39m}[platform\u001b[38;5;241m.\u001b[39msystem()]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Linux, Windows, MinGW\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMODEL_LIB_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibllmodel.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdll\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/rlagusrb/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.so)"
     ]
    }
   ],
   "source": [
    "# 8.3 GPT4All\n",
    "\n",
    "from langchain.llms.gpt4all import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that defines words. Define this word: {word}.\"\n",
    ")\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=\"./falcon.bin\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Ollama\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"PrivateGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/private_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/private_embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral:latest\")\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question using ONLY the following context and not your training data. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question:{question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"PrivateGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        with st.chat_message(\"ai\"):\n",
    "            chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 WikipediaRetriever\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import streamlit as st\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"QuizGPT\",\n",
    "    page_icon=\"❓\",\n",
    ")\n",
    "\n",
    "st.title(\"QuizGPT\")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading file...\")\n",
    "def split_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/quiz_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    return docs\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    choice = st.selectbox(\n",
    "        \"Choose what you want to use.\",\n",
    "        (\n",
    "            \"File\",\n",
    "            \"Wikipedia Article\",\n",
    "        ),\n",
    "    )\n",
    "    if choice == \"File\":\n",
    "        file = st.file_uploader(\n",
    "            \"Upload a .docx , .txt or .pdf file\",\n",
    "            type=[\"pdf\", \"txt\", \"docx\"],\n",
    "        )\n",
    "        if file:\n",
    "            docs = split_file(file)\n",
    "            st.write(docs)\n",
    "    else:\n",
    "        topic = st.text_input(\"Search Wikipedia...\")\n",
    "        if topic:\n",
    "            retriever = WikipediaRetriever(top_k_results=5)\n",
    "            with st.status(\"Searching Wikipedia...\"):\n",
    "                docs = retriever.get_relevant_documents(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 GPT-4 Turbo\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import streamlit as st\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"QuizGPT\",\n",
    "    page_icon=\"❓\",\n",
    ")\n",
    "\n",
    "st.title(\"QuizGPT\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading file...\")\n",
    "def split_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/quiz_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    return docs\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    docs = None\n",
    "    choice = st.selectbox(\n",
    "        \"Choose what you want to use.\",\n",
    "        (\n",
    "            \"File\",\n",
    "            \"Wikipedia Article\",\n",
    "        ),\n",
    "    )\n",
    "    if choice == \"File\":\n",
    "        file = st.file_uploader(\n",
    "            \"Upload a .docx , .txt or .pdf file\",\n",
    "            type=[\"pdf\", \"txt\", \"docx\"],\n",
    "        )\n",
    "        if file:\n",
    "            docs = split_file(file)\n",
    "    else:\n",
    "        topic = st.text_input(\"Search Wikipedia...\")\n",
    "        if topic:\n",
    "            retriever = WikipediaRetriever(top_k_results=5)\n",
    "            with st.status(\"Searching Wikipedia...\"):\n",
    "                docs = retriever.get_relevant_documents(topic)\n",
    "\n",
    "\n",
    "if not docs:\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    Welcome to QuizGPT.\n",
    "                \n",
    "    I will make a quiz from Wikipedia articles or files you upload to test your knowledge and help you study.\n",
    "                \n",
    "    Get started by uploading a file or searching on Wikipedia in the sidebar.\n",
    "    \"\"\"\n",
    "    )\n",
    "else:\n",
    "    st.write(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Questions Prompt\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "import streamlit as st\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"QuizGPT\",\n",
    "    page_icon=\"❓\",\n",
    ")\n",
    "\n",
    "st.title(\"QuizGPT\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading file...\")\n",
    "def split_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/quiz_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    docs = None\n",
    "    choice = st.selectbox(\n",
    "        \"Choose what you want to use.\",\n",
    "        (\n",
    "            \"File\",\n",
    "            \"Wikipedia Article\",\n",
    "        ),\n",
    "    )\n",
    "    if choice == \"File\":\n",
    "        file = st.file_uploader(\n",
    "            \"Upload a .docx , .txt or .pdf file\",\n",
    "            type=[\"pdf\", \"txt\", \"docx\"],\n",
    "        )\n",
    "        if file:\n",
    "            docs = split_file(file)\n",
    "    else:\n",
    "        topic = st.text_input(\"Search Wikipedia...\")\n",
    "        if topic:\n",
    "            retriever = WikipediaRetriever(top_k_results=5)\n",
    "            with st.status(\"Searching Wikipedia...\"):\n",
    "                docs = retriever.get_relevant_documents(topic)\n",
    "\n",
    "\n",
    "if not docs:\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    Welcome to QuizGPT.\n",
    "                \n",
    "    I will make a quiz from Wikipedia articles or files you upload to test your knowledge and help you study.\n",
    "                \n",
    "    Get started by uploading a file or searching on Wikipedia in the sidebar.\n",
    "    \"\"\"\n",
    "    )\n",
    "else:\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "    You are a helpful assistant that is role playing as a teacher.\n",
    "         \n",
    "    Based ONLY on the following context make 10 questions to test the user's knowledge about the text.\n",
    "    \n",
    "    Each question should have 4 answers, three of them must be incorrect and one should be correct.\n",
    "         \n",
    "    Use (o) to signal the correct answer.\n",
    "         \n",
    "    Question examples:\n",
    "         \n",
    "    Question: What is the color of the ocean?\n",
    "    Answers: Red|Yellow|Green|Blue(o)\n",
    "         \n",
    "    Question: What is the capital or Georgia?\n",
    "    Answers: Baku|Tbilisi(o)|Manila|Beirut\n",
    "         \n",
    "    Question: When was Avatar released?\n",
    "    Answers: 2007|2001|2009(o)|1998\n",
    "         \n",
    "    Question: Who was Julius Caesar?\n",
    "    Answers: A Roman Emperor(o)|Painter|Actor|Model\n",
    "         \n",
    "    Your turn!\n",
    "         \n",
    "    Context: {context}\n",
    "\"\"\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = {\"context\": format_docs} | prompt | llm\n",
    "\n",
    "    start = st.button(\"Generate Quiz\")\n",
    "\n",
    "    if start:\n",
    "        chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 Formatter Prompt\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "import streamlit as st\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"QuizGPT\",\n",
    "    page_icon=\"❓\",\n",
    ")\n",
    "\n",
    "st.title(\"QuizGPT\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "questions_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "    You are a helpful assistant that is role playing as a teacher.\n",
    "         \n",
    "    Based ONLY on the following context make 10 questions to test the user's knowledge about the text.\n",
    "    \n",
    "    Each question should have 4 answers, three of them must be incorrect and one should be correct.\n",
    "         \n",
    "    Use (o) to signal the correct answer.\n",
    "         \n",
    "    Question examples:\n",
    "         \n",
    "    Question: What is the color of the ocean?\n",
    "    Answers: Red|Yellow|Green|Blue(o)\n",
    "         \n",
    "    Question: What is the capital or Georgia?\n",
    "    Answers: Baku|Tbilisi(o)|Manila|Beirut\n",
    "         \n",
    "    Question: When was Avatar released?\n",
    "    Answers: 2007|2001|2009(o)|1998\n",
    "         \n",
    "    Question: Who was Julius Caesar?\n",
    "    Answers: A Roman Emperor(o)|Painter|Actor|Model\n",
    "         \n",
    "    Your turn!\n",
    "         \n",
    "    Context: {context}\n",
    "\"\"\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "questions_chain = {\"context\": format_docs} | questions_prompt | llm\n",
    "\n",
    "formatting_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "    You are a powerful formatting algorithm.\n",
    "     \n",
    "    You format exam questions into JSON format.\n",
    "    Answers with (o) are the correct ones.\n",
    "     \n",
    "    Example Input:\n",
    "\n",
    "    Question: What is the color of the ocean?\n",
    "    Answers: Red|Yellow|Green|Blue(o)\n",
    "         \n",
    "    Question: What is the capital or Georgia?\n",
    "    Answers: Baku|Tbilisi(o)|Manila|Beirut\n",
    "         \n",
    "    Question: When was Avatar released?\n",
    "    Answers: 2007|2001|2009(o)|1998\n",
    "         \n",
    "    Question: Who was Julius Caesar?\n",
    "    Answers: A Roman Emperor(o)|Painter|Actor|Model\n",
    "    \n",
    "     \n",
    "    Example Output:\n",
    "     \n",
    "    ```json\n",
    "    {{ \"questions\": [\n",
    "            {{\n",
    "                \"question\": \"What is the color of the ocean?\",\n",
    "                \"answers\": [\n",
    "                        {{\n",
    "                            \"answer\": \"Red\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Yellow\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Green\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Blue\",\n",
    "                            \"correct\": true\n",
    "                        }},\n",
    "                ]\n",
    "            }},\n",
    "                        {{\n",
    "                \"question\": \"What is the capital or Georgia?\",\n",
    "                \"answers\": [\n",
    "                        {{\n",
    "                            \"answer\": \"Baku\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Tbilisi\",\n",
    "                            \"correct\": true\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Manila\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Beirut\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                ]\n",
    "            }},\n",
    "                        {{\n",
    "                \"question\": \"When was Avatar released?\",\n",
    "                \"answers\": [\n",
    "                        {{\n",
    "                            \"answer\": \"2007\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"2001\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"2009\",\n",
    "                            \"correct\": true\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"1998\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                ]\n",
    "            }},\n",
    "            {{\n",
    "                \"question\": \"Who was Julius Caesar?\",\n",
    "                \"answers\": [\n",
    "                        {{\n",
    "                            \"answer\": \"A Roman Emperor\",\n",
    "                            \"correct\": true\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Painter\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Actor\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"answer\": \"Model\",\n",
    "                            \"correct\": false\n",
    "                        }},\n",
    "                ]\n",
    "            }}\n",
    "        ]\n",
    "     }}\n",
    "    ```\n",
    "    Your turn!\n",
    "\n",
    "    Questions: {context}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatting_chain = formatting_prompt | llm\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading file...\")\n",
    "def split_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/quiz_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    return docs\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    docs = None\n",
    "    choice = st.selectbox(\n",
    "        \"Choose what you want to use.\",\n",
    "        (\n",
    "            \"File\",\n",
    "            \"Wikipedia Article\",\n",
    "        ),\n",
    "    )\n",
    "    if choice == \"File\":\n",
    "        file = st.file_uploader(\n",
    "            \"Upload a .docx , .txt or .pdf file\",\n",
    "            type=[\"pdf\", \"txt\", \"docx\"],\n",
    "        )\n",
    "        if file:\n",
    "            docs = split_file(file)\n",
    "    else:\n",
    "        topic = st.text_input(\"Search Wikipedia...\")\n",
    "        if topic:\n",
    "            retriever = WikipediaRetriever(top_k_results=5)\n",
    "            with st.status(\"Searching Wikipedia...\"):\n",
    "                docs = retriever.get_relevant_documents(topic)\n",
    "\n",
    "\n",
    "if not docs:\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    Welcome to QuizGPT.\n",
    "                \n",
    "    I will make a quiz from Wikipedia articles or files you upload to test your knowledge and help you study.\n",
    "                \n",
    "    Get started by uploading a file or searching on Wikipedia in the sidebar.\n",
    "    \"\"\"\n",
    "    )\n",
    "else:\n",
    "    start = st.button(\"Generate Quiz\")\n",
    "\n",
    "    if start:\n",
    "        questions_response = questions_chain.invoke(docs)\n",
    "        st.write(questions_response.content)\n",
    "        formatting_response = formatting_chain.invoke(\n",
    "            {\"context\": questions_response.content}\n",
    "        )\n",
    "        st.write(formatting_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 Output Parser\n",
    "\n",
    "import json\n",
    "from operator import rshift\n",
    "from langchain.schema import BaseOutputParser, output_parser\n",
    "\n",
    "\n",
    "class JsonOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        text = text.replace(\"```\", \"\").replace(\"json\", \"\")\n",
    "        return json.loads(text)\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "if start:\n",
    "        st.write(formatting_response.content)\n",
    "        chain = {\"context\": questions_chain} | formatting_chain | output_parser\n",
    "        response = chain.invoke(docs)\n",
    "        st.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.6 Caching\n",
    "\n",
    "@st.cache_data(show_spinner=\"Making quiz...\")\n",
    "def run_quiz_chain(_docs, topic):\n",
    "    chain = {\"context\": questions_chain} | formatting_chain | output_parser\n",
    "    return chain.invoke(_docs)\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Searching Wikipedia...\")\n",
    "def wiki_search(term):\n",
    "    retriever = WikipediaRetriever(top_k_results=5)\n",
    "    docs = retriever.get_relevant_documents(term)\n",
    "    return docs\n",
    "\n",
    "topic = st.text_input(\"Search Wikipedia...\")\n",
    "if topic:\n",
    "    docs = wiki_search(topic)\n",
    "\n",
    "if start:\n",
    "        response = run_quiz_chain(docs, topic if topic else file.name)\n",
    "        st.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.7 Grading Questions\n",
    "\n",
    "response = run_quiz_chain(docs, topic if topic else file.name)\n",
    "with st.form(\"questions_form\"):\n",
    "    for question in response[\"questions\"]:\n",
    "        st.write(question[\"question\"])\n",
    "        value = st.radio(\n",
    "            \"Select an option.\",\n",
    "            [answer[\"answer\"] for answer in question[\"answers\"]],\n",
    "            index=None,\n",
    "        )\n",
    "        if {\"answer\": value, \"correct\": True} in question[\"answers\"]:\n",
    "            st.success(\"Correct!\")\n",
    "        elif value is not None:\n",
    "            st.error(\"Wrong!\")\n",
    "    button = st.form_submit_button()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      7\u001b[0m function \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_quiz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction that takes a list of questions and answers and returns a quiz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     },\n\u001b[1;32m     43\u001b[0m }\n\u001b[0;32m---> 46\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m     49\u001b[0m     function_call\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_quiz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     },\n\u001b[1;32m     52\u001b[0m     functions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     53\u001b[0m         function,\n\u001b[1;32m     54\u001b[0m     ],\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake a quiz about \u001b[39m\u001b[38;5;132;01m{city}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/FullStack-GPT/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# 9.8 Function Calling\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "function = {\n",
    "    \"name\": \"create_quiz\",\n",
    "    \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ").bind(\n",
    "    function_call={\n",
    "        \"name\": \"create_quiz\",\n",
    "    },\n",
    "    functions=[\n",
    "        function,\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 AsyncChromiumLoader\n",
    "\n",
    "with st.sidebar:\n",
    "    url = st.text_input(\n",
    "        \"Write down a URL\",\n",
    "        placeholder=\"https://example.com\",\n",
    "    )\n",
    "\n",
    "\n",
    "if url:\n",
    "    loader = AsyncChromiumLoader([url])\n",
    "    docs = loader.load()\n",
    "    transformed = html2text_transformer.transform_documents(docs)\n",
    "    st.write(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 SitemapLoader\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading website...\")\n",
    "def load_website(url):\n",
    "    loader = SitemapLoader(url)\n",
    "    loader.requests_per_second = 5\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "if url:\n",
    "    if \".xml\" not in url:\n",
    "        with st.sidebar:\n",
    "            st.error(\"Please write down a Sitemap URL.\")\n",
    "    else:\n",
    "        docs = load_website(url)\n",
    "        st.write(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Parsing Function\n",
    "\n",
    "from langchain.document_loaders import SitemapLoader, text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def parse_page(soup):\n",
    "    header = soup.find(\"header\") # header tag 찾기\n",
    "    footer = soup.find(\"footer\")\n",
    "    if header:\n",
    "# .decompose() : beautifulsoup 메서드, HTML tree로부터 Tag와 content 분리해서 제거\n",
    "        header.decompose()\n",
    "    if footer:\n",
    "        footer.decompose()\n",
    "    return (\n",
    "        str(soup.get_text()) # get_text : beatifulsoup 메서드, 특정 tag 문서 읽기\n",
    "        .replace(\"\\n\", \" \") # 줄바꿈 제거\n",
    "        .replace(\"\\xa0\", \" \") # 공백문자 제거\n",
    "        .replace(\"CloseSearch Submit Blog\", \"\")\n",
    "    )\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading website...\")\n",
    "def load_website(url):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    loader = SitemapLoader(\n",
    "        url,\n",
    "        filter_urls=[\n",
    "            r\"^(.*\\/blog\\/).*\",\n",
    "        ],\n",
    "        parsing_function=parse_page,\n",
    "    )\n",
    "    loader.requests_per_second = 2\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.4 Map Re Rank Chain\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "answers_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Using ONLY the following context answer the user's question. If you can't just say you don't know, don't make anything up.\n",
    "                                                  \n",
    "    Then, give a score to the answer between 0 and 5.\n",
    "    If the answer answers the user question the score should be high, else it should be low.\n",
    "    Make sure to always include the answer's score even if it's 0.\n",
    "    Context: {context}\n",
    "                                                  \n",
    "    Examples:\n",
    "                                                  \n",
    "    Question: How far away is the moon?\n",
    "    Answer: The moon is 384,400 km away.\n",
    "    Score: 5\n",
    "                                                  \n",
    "    Question: How far away is the sun?\n",
    "    Answer: I don't know\n",
    "    Score: 0\n",
    "                                                  \n",
    "    Your turn!\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_answers(inputs):\n",
    "    docs = inputs[\"docs\"]\n",
    "    question = inputs[\"question\"]\n",
    "    answers_chain = answers_prompt | llm\n",
    "    answers = []\n",
    "    for doc in docs:\n",
    "        result = answers_chain.invoke(\n",
    "            {\"question\": question, \"context\": doc.page_content}\n",
    "        )\n",
    "        answers.append(result.content)\n",
    "    st.write(answers)\n",
    "\n",
    "@st.cache_data(show_spinner=\"Loading website...\")\n",
    "def load_website(url):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    loader = SitemapLoader(\n",
    "        url,\n",
    "        parsing_function=parse_page,\n",
    "    )\n",
    "    loader.requests_per_second = 2\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "    return vector_store.as_retriever()\n",
    "\n",
    "if url:\n",
    "    if \".xml\" not in url:\n",
    "        with st.sidebar:\n",
    "            st.error(\"Please write down a Sitemap URL.\")\n",
    "    else:\n",
    "        retriever = load_website(url)\n",
    "\n",
    "        chain = {\n",
    "            \"docs\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        } | RunnableLambda(get_answers)\n",
    "\n",
    "        chain.invoke(\"What is the pricing of GPT-4 Turbo with vision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 Map Re Rank Chain part Two\n",
    "\n",
    "def get_answers(inputs):\n",
    "    docs = inputs[\"docs\"]\n",
    "    question = inputs[\"question\"]\n",
    "    answers_chain = answers_prompt | llm\n",
    "    # answers = []\n",
    "    # for doc in docs:\n",
    "    #     result = answers_chain.invoke(\n",
    "    #         {\"question\": question, \"context\": doc.page_content}\n",
    "    #     )\n",
    "    #     answers.append(result.content)\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"answer\": answers_chain.invoke(\n",
    "                    {\"question\": question, \"context\": doc.page_content}\n",
    "                ).content,\n",
    "                \"source\": doc.metadata[\"source\"],\n",
    "                \"date\": doc.metadata[\"lastmod\"],\n",
    "            }\n",
    "            for doc in docs\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def choose_answer(inputs):\n",
    "    answers = inputs[\"answers\"]\n",
    "    question = inputs[\"question\"]\n",
    "    choose_chain = choose_prompt | llm\n",
    "    condensed = \"\\n\\n\".join(\n",
    "        f\"{answer['answer']}\\nSource:{answer['source']}\\nDate:{answer['date']}\\n\"\n",
    "        for answer in answers\n",
    "    )\n",
    "    return choose_chain.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answers\": condensed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "choose_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use ONLY the following pre-existing answers to answer the user's question.\n",
    "            Use the answers that have the highest score (more helpful) and favor the most recent ones.\n",
    "            Cite sources and return the sources of the answers as they are, do not change them.\n",
    "            Answers: {answers}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if url:\n",
    "    if \".xml\" not in url:\n",
    "        with st.sidebar:\n",
    "            st.error(\"Please write down a Sitemap URL.\")\n",
    "    else:\n",
    "        retriever = load_website(url)\n",
    "        query = st.text_input(\"Ask a question to the website.\")\n",
    "        if query:\n",
    "            chain = (\n",
    "                {\n",
    "                    \"docs\": retriever,\n",
    "                    \"question\": RunnablePassthrough(),\n",
    "                }\n",
    "                | RunnableLambda(get_answers)\n",
    "                | RunnableLambda(choose_answer)\n",
    "            )\n",
    "            result = chain.invoke(query)\n",
    "            st.markdown(result.content.replace(\"$\", \"\\$\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FullStack-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
